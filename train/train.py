import os
import tensorflow as tf
import pandas as pd
from input import input_fn_builder
from tqdm import tqdm
import sys

flags = tf.flags
FLAGS = flags.FLAGS

# flags.DEFINE_string("output_dir", "model", "The output directory.")
flags.DEFINE_string("assets_dir", "assets", "The assets directory generated by assets.py.")
flags.DEFINE_string("checkpoint_dir", 'ckpt', "The directory for storing model check points.")
flags.DEFINE_string("bert_model_ch_dir", None, "A google storage path to unzipped BERT chinese_L-12_H-768_A-12 model.")
flags.DEFINE_string("train_file", "dataset/train_128.tfrecords", "The training file output by dataset.py.")
flags.DEFINE_string("val_file", "dataset/val_128.tfrecords", "The validation file output by dataset.py.")
flags.DEFINE_integer("max_seq_length", 128, "Maximum sequence length.")

flags.DEFINE_float("learning_rate", 2e-5, "The learning rate.")
flags.DEFINE_integer("train_steps_per_epoch", 800, "Number of training steps per epoch.")
flags.DEFINE_integer("val_steps_per_epoch", 50, "Number of validation steps per epoch.")
flags.DEFINE_float("warmup_proportion", 0.1, "")
flags.DEFINE_integer("num_epoch", 10, "Number of epoch to be trained.")
flags.DEFINE_integer("save_checkpoints_steps", 800, "Number of steps to save a checkpoint.")
flags.DEFINE_integer("batch_size", 64, "The training and validation batch size.")

flags.DEFINE_bool("use_tpu", False, "Use TPU for training.")
flags.DEFINE_integer("num_tpu_cores", 8, "")
flags.DEFINE_string("tpu_name", "localhost", "")

def main(_):
    tf.logging.set_verbosity(tf.logging.INFO)
    # tf.gfile.MakeDirs(FLAGS.output_dir)

    sys.path += [os.path.join(FLAGS.assets_dir, 'bert')]
    from model import model_fn_builder
    import modeling

    train_input_fn = input_fn_builder(
        input_file=FLAGS.train_file,
        seq_length=FLAGS.max_seq_length,
        shuffle=True,
        repeat=True,
        drop_remainder=FLAGS.use_tpu
    )

    val_input_fn = input_fn_builder(
        input_file=FLAGS.val_file,
        seq_length=FLAGS.max_seq_length,
        shuffle=False,
        repeat=False,
        drop_remainder=FLAGS.use_tpu
    )

    model_fn = model_fn_builder(
        bert_config=modeling.BertConfig.from_json_file(
            os.path.join(FLAGS.bert_model_ch_dir, 'bert_config.json')
        ),
        init_checkpoint=os.path.join(FLAGS.bert_model_ch_dir, 'bert_model.ckpt'),
        use_tpu=FLAGS.use_tpu,
        use_one_hot_embeddings=True if FLAGS.use_tpu else False,
        learning_rate=FLAGS.learning_rate,
        num_train_steps=FLAGS.train_steps_per_epoch,
        num_warmup_steps=int(FLAGS.train_steps_per_epoch * FLAGS.warmup_proportion)
    )

    run_config = tf.contrib.tpu.RunConfig(
        cluster=tf.contrib.cluster_resolver.TPUClusterResolver(FLAGS.tpu_name) if FLAGS.use_tpu else None,
        model_dir=FLAGS.checkpoint_dir,
        save_checkpoints_steps=FLAGS.train_steps_per_epoch,
        tpu_config=tf.contrib.tpu.TPUConfig(
            iterations_per_loop=FLAGS.train_steps_per_epoch,
            num_shards=FLAGS.num_tpu_cores,
            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2
        )
    )

    estimator = tf.contrib.tpu.TPUEstimator(
        use_tpu=FLAGS.use_tpu,
        model_fn=model_fn,
        config=run_config,
        train_batch_size=FLAGS.batch_size,
        eval_batch_size=FLAGS.batch_size
    )

    tf.logging.info('Setup success...')

    for i in range(FLAGS.num_epoch):
        tf.logging.info('Epoch: %d/%d'%(i, FLAGS.num_epoch))
        # estimator.train(
        #     input_fn=train_input_fn,
        #     steps=FLAGS.train_steps_per_epoch,
        # )

        estimator.evaluate(
            input_fn=val_input_fn,
            steps=FLAGS.val_steps_per_epoch,
        )


if __name__ == "__main__":
  tf.app.run()
